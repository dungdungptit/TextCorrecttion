{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eng_to_ipa import convert\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def compare_ipa(transcript: str, reference_text: str) -> list:\n",
    "    \"\"\"\n",
    "    So sánh IPA của transcript với bản tham chiếu.\n",
    "    Trả về danh sách lỗi phát âm IPA.\n",
    "    \"\"\"\n",
    "    transcript_words = transcript.split()\n",
    "    reference_words = reference_text.split()\n",
    "    \n",
    "    ipa_errors = []\n",
    "\n",
    "    # So sánh từng từ\n",
    "    for ref_word, spoken_word in zip(reference_words, transcript_words):\n",
    "        ref_ipa = convert(ref_word)  # Chuyển từ tham chiếu sang IPA\n",
    "        spoken_ipa = convert(spoken_word)  # Chuyển từ transcript sang IPA\n",
    "\n",
    "        # Tính mức độ tương đồng giữa IPA chuẩn và thực tế\n",
    "        similarity = SequenceMatcher(None, ref_ipa, spoken_ipa).ratio()\n",
    "\n",
    "        if similarity < 0.9:  # Ngưỡng sai phát âm (90% tương đồng)\n",
    "            ipa_errors.append({\n",
    "                \"word\": spoken_word,\n",
    "                \"reference_ipa\": ref_ipa,\n",
    "                \"spoken_ipa\": spoken_ipa,\n",
    "                \"similarity\": round(similarity * 100, 2)  # Tính phần trăm tương đồng\n",
    "            })\n",
    "    \n",
    "    return ipa_errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pronunciation_tips(ipa_errors: list) -> list:\n",
    "    \"\"\"Sinh gợi ý cải thiện phát âm từ lỗi IPA.\"\"\"\n",
    "    tips = []\n",
    "    for error in ipa_errors:\n",
    "        tips.append({\n",
    "            \"word\": error[\"word\"],\n",
    "            \"tip\": f\"Hãy phát âm '{error['word']}' chính xác như: /{error['reference_ipa']}/\"\n",
    "        })\n",
    "    return tips\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript = \"This is a test\"\n",
    "reference_text = \"This is a test\"\n",
    "# Phân tích lỗi IPA\n",
    "ipa_errors = compare_ipa(transcript, reference_text)\n",
    "\n",
    "# Gợi ý cải thiện phát âm\n",
    "pronunciation_tips = generate_pronunciation_tips(ipa_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ipa_errors': [], 'pronunciation_tips': []}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trả về kết quả\n",
    "response = {\n",
    "    \"ipa_errors\": ipa_errors,\n",
    "    \"pronunciation_tips\": pronunciation_tips\n",
    "}\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import language_tool_python\n",
    "\n",
    "def calculate_grammar_score(transcript: str) -> float:\n",
    "    \"\"\"Tính điểm Grammar dựa trên số lỗi.\"\"\"\n",
    "    tool = language_tool_python.LanguageTool(\"en-US\")\n",
    "    matches = tool.check(transcript)\n",
    "    total_words = len(transcript.split())\n",
    "    grammar_errors = len(matches)\n",
    "    \n",
    "    # Tính điểm (100% nếu không có lỗi)\n",
    "    grammar_score = max(0, 100 - (grammar_errors / total_words) * 100)\n",
    "    return grammar_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "def calculate_vocabulary_score(transcript: str, reference_text: str) -> float:\n",
    "    \"\"\"Tính điểm Vocabulary dựa trên từ đúng và sai.\"\"\"\n",
    "    transcript_words = set(transcript.lower().split())\n",
    "    reference_words = set(reference_text.lower().split())\n",
    "    \n",
    "    # Tính số từ đúng và sai\n",
    "    correct_words = transcript_words.intersection(reference_words)\n",
    "    vocab_score = (len(correct_words) / len(reference_words)) * 100 if reference_words else 0\n",
    "    return vocab_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fluency_score(segments: list) -> float:\n",
    "    \"\"\"Tính điểm Fluency dựa trên số lần dừng và độ dài dừng.\"\"\"\n",
    "    pauses = [segments[i]['start'] - segments[i - 1]['end'] for i in range(1, len(segments))]\n",
    "    long_pauses = [pause for pause in pauses if pause > 0.5]  # Dừng dài hơn 0.5s\n",
    "    \n",
    "    # Điểm fluency dựa trên số lần dừng (ít dừng = điểm cao)\n",
    "    total_pauses = len(pauses)\n",
    "    fluency_score = max(0, 100 - (len(long_pauses) / total_pauses) * 100 if total_pauses > 0 else 0)\n",
    "    return fluency_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pronunciation_score(transcript: str, ipa_reference: list, ipa_transcript: list) -> float:\n",
    "    \"\"\"Tính điểm Pronunciation dựa trên sự khớp IPA.\"\"\"\n",
    "    correct_pronunciation = sum(1 for ref, spoken in zip(ipa_reference, ipa_transcript) if ref == spoken)\n",
    "    total_pronunciation = len(ipa_reference)\n",
    "    \n",
    "    # Điểm phát âm dựa trên tỷ lệ khớp\n",
    "    pronunciation_score = (correct_pronunciation / total_pronunciation) * 100 if total_pronunciation > 0 else 0\n",
    "    return pronunciation_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_overall_score(transcript: str, reference_text: str, segments: list, ipa_reference: list, ipa_transcript: list):\n",
    "    \"\"\"Tính điểm tổng hợp.\"\"\"\n",
    "    grammar_score = calculate_grammar_score(transcript)\n",
    "    vocabulary_score = calculate_vocabulary_score(transcript, reference_text)\n",
    "    fluency_score = calculate_fluency_score(segments)\n",
    "    pronunciation_score = calculate_pronunciation_score(transcript, ipa_reference, ipa_transcript)\n",
    "    \n",
    "    # Tính điểm trung bình\n",
    "    overall_score = (grammar_score + vocabulary_score + fluency_score + pronunciation_score) / 4\n",
    "    \n",
    "    return {\n",
    "        \"grammar\": grammar_score,\n",
    "        \"vocabulary\": vocabulary_score,\n",
    "        \"fluency\": fluency_score,\n",
    "        \"pronunciation\": pronunciation_score,\n",
    "        \"overall\": overall_score\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\pronunc\\Lib\\site-packages\\whisper\\__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Whisper(\n",
       "  (encoder): AudioEncoder(\n",
       "    (conv1): Conv1d(80, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (conv2): Conv1d(768, 768, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (blocks): ModuleList(\n",
       "      (0-11): 12 x ResidualAttentionBlock(\n",
       "        (attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (mlp_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): TextDecoder(\n",
       "    (token_embedding): Embedding(51865, 768)\n",
       "    (blocks): ModuleList(\n",
       "      (0-11): 12 x ResidualAttentionBlock(\n",
       "        (attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (cross_attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (mlp_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import whisper\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "whisper.load_model('small').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\pronunc\\Lib\\site-packages\\whisper\\__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n",
      "c:\\Users\\Admin\\anaconda3\\envs\\pronunc\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript:  How are you doing nowadays?\n"
     ]
    }
   ],
   "source": [
    "def transcribe_audio_whisper(audio_file):\n",
    "    \"\"\"Sử dụng Whisper để nhận diện giọng nói.\"\"\"\n",
    "    model = whisper.load_model(\"small\")  # Tải mô hình Whisper Small\n",
    "    result = model.transcribe(audio_file)\n",
    "    transcript = result['text']\n",
    "    segments = result['segments']  # Lấy thời gian từng đoạn\n",
    "    print(f\"Transcript: {transcript}\")\n",
    "    return transcript, segments\n",
    "\n",
    "# Kiểm tra\n",
    "audio_file = \"./IELTS_PracticeAndEvaluation/test_data/how-are-you-doing-now-a-days.wav\"\n",
    "transcript, segments = transcribe_audio_whisper(audio_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(' How are you doing nowadays?',\n",
       " [{'id': 0,\n",
       "   'seek': 0,\n",
       "   'start': 0.0,\n",
       "   'end': 2.0,\n",
       "   'text': ' How are you doing nowadays?',\n",
       "   'tokens': [50364, 1012, 366, 291, 884, 13434, 30, 50464],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.37167814042833114,\n",
       "   'compression_ratio': 0.7714285714285715,\n",
       "   'no_speech_prob': 0.009825356304645538}])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gọi Whisper để lấy transcript và segments\n",
    "transcript, segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' How are you doing nowadays?'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\pronunc\\Lib\\site-packages\\whisper\\__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n",
      "c:\\Users\\Admin\\anaconda3\\envs\\pronunc\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript:  How are you doing nowadays?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' How are you doing nowadays?'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcribe_audio_whisper(audio_file)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_overall_score_with_ipa(transcript: str, reference_text: str, segments: list, ipa_errors: list):\n",
    "    \"\"\"Tính điểm và thêm lỗi phát âm IPA.\"\"\"\n",
    "    grammar_score = calculate_grammar_score(transcript)\n",
    "    vocabulary_score = calculate_vocabulary_score(transcript, reference_text)\n",
    "    fluency_score = calculate_fluency_score(segments)\n",
    "    pronunciation_score = 100 - len(ipa_errors)  # Giảm điểm phát âm nếu có lỗi IPA\n",
    "    \n",
    "    # Tính điểm trung bình\n",
    "    overall_score = (grammar_score + vocabulary_score + fluency_score + pronunciation_score) / 4\n",
    "\n",
    "    return {\n",
    "        \"grammar\": grammar_score,\n",
    "        \"vocabulary\": vocabulary_score,\n",
    "        \"fluency\": fluency_score,\n",
    "        \"pronunciation\": pronunciation_score,\n",
    "        \"overall\": overall_score,\n",
    "        \"ipa_errors\": ipa_errors  # Thêm chi tiết lỗi IPA\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'grammar': 100.0,\n",
       " 'vocabulary': 75.0,\n",
       " 'fluency': 0,\n",
       " 'pronunciation': 100,\n",
       " 'overall': 68.75,\n",
       " 'ipa_errors': []}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = calculate_overall_score_with_ipa(transcript, \"how are you going\", segments, ipa_errors)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "from eng_to_ipa import convert as text_to_ipa\n",
    "import markdown\n",
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Remove punctuation and normalize text for comparison\"\"\"\n",
    "    return re.sub(r'[^\\w\\s]', '', text).strip().lower()\n",
    "\n",
    "def highlight_differences(reference, actual):\n",
    "    \"\"\"Highlight differences between reference and actual IPA transcriptions\"\"\"\n",
    "    diff = difflib.SequenceMatcher(None, reference, actual)\n",
    "    highlighted_reference = []\n",
    "    highlighted_actual = []\n",
    "\n",
    "    for tag, i1, i2, j1, j2 in diff.get_opcodes():\n",
    "        ref_segment = reference[i1:i2]\n",
    "        act_segment = actual[j1:j2]\n",
    "\n",
    "        if tag == 'equal':\n",
    "            highlighted_reference.append(ref_segment)\n",
    "            highlighted_actual.append(act_segment)\n",
    "        elif tag in ('replace', 'delete'):\n",
    "            highlighted_reference.append(f\"**{ref_segment}**\")\n",
    "        if tag in ('replace', 'insert'):\n",
    "            highlighted_actual.append(f\"**{act_segment}**\")\n",
    "\n",
    "    return \"\".join(highlighted_reference), \"\".join(highlighted_actual)\n",
    "\n",
    "def evaluate_pronunciation(input_text, audio_path):\n",
    "    \"\"\"Evaluate pronunciation and return Markdown-formatted result\"\"\"\n",
    "    # Convert input text to IPA\n",
    "    expected_ipa = text_to_ipa(input_text)\n",
    "\n",
    "    # Transcribe audio to text using Whisper\n",
    "    transcribed_text = transcribe_audio_whisper(audio_path)[0]\n",
    "    normalized_transcribed_text = preprocess_text(transcribed_text)\n",
    "\n",
    "    # Convert Whisper transcription to IPA\n",
    "    transcribed_ipa = text_to_ipa(transcribed_text)\n",
    "\n",
    "    # Highlight differences in IPA\n",
    "    highlighted_expected, highlighted_actual = highlight_differences(expected_ipa, transcribed_ipa)\n",
    "\n",
    "    # Highlight differences in the original text\n",
    "    highlighted_text, highlighted_transcription = highlight_differences(input_text, transcribed_text)\n",
    "\n",
    "    # Create Markdown output\n",
    "    markdown_output = f\"\"\"### Pronunciation Evaluation\n",
    "\n",
    "#### Original Text:\n",
    "{highlighted_text}\n",
    "\n",
    "#### Transcribed Text:\n",
    "{highlighted_transcription}\n",
    "\n",
    "#### Expected IPA:\n",
    "{highlighted_expected}\n",
    "\n",
    "#### Transcribed IPA:\n",
    "{highlighted_actual}\n",
    "\"\"\"\n",
    "    return markdown_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript:  Why hello there.\n",
      "### Pronunciation Evaluation\n",
      "\n",
      "#### Original Text:\n",
      "**w**hy hello here\n",
      "\n",
      "#### Transcribed Text:\n",
      "** W**hy hello **t**here**.**\n",
      "\n",
      "#### Expected IPA:\n",
      "waɪ hɛˈloʊ **hi**r\n",
      "\n",
      "#### Transcribed IPA:\n",
      "waɪ hɛˈloʊ **ðɛ**r**.**\n",
      "\n",
      "Pronunciation evaluation saved to pronunciation_evaluation.md\n"
     ]
    }
   ],
   "source": [
    "input_text = \"why hello here\"  # Replace with the user's text\n",
    "audio_path = \"./IELTS_PracticeAndEvaluation/test_data/why-hello-there-103596.wav\"  # Replace with the path to the user's audio file\n",
    "\n",
    "result = evaluate_pronunciation(input_text, audio_path)\n",
    "print(result)\n",
    "# Save the result to a Markdown file\n",
    "with open(\"pronunciation_evaluation.md\", \"w\") as f:\n",
    "    f.write(result)\n",
    "\n",
    "print(\"Pronunciation evaluation saved to pronunciation_evaluation.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transcript:  Why hello there.\n",
    "### Pronunciation Evaluation\n",
    "\n",
    "#### Original Text:\n",
    "**w**hy hello here\n",
    "\n",
    "#### Transcribed Text:\n",
    "** W**hy hello **t**here**.**\n",
    "\n",
    "#### Expected IPA:\n",
    "waɪ hɛˈloʊ **hi**r\n",
    "\n",
    "#### Transcribed IPA:\n",
    "waɪ hɛˈloʊ **ðɛ**r**.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript:  Why hello there.\n",
      "\n",
      "### Pronunciation Feedback\n",
      "\n",
      "#### Text with Mistakes Highlighted:\n",
      "why hello  **h**  **e**  **r**  **e** \n",
      "\n",
      "#### Correct IPA:\n",
      "/ waɪ hɛˈloʊ hir /\n",
      "\n",
      "#### Transcribed IPA:\n",
      "/ waɪ hɛˈloʊ  **ðɛ** r /\n",
      "\n",
      "Pronunciation evaluation saved to pronunciation_evaluation.md\n"
     ]
    }
   ],
   "source": [
    "import difflib\n",
    "from eng_to_ipa import convert as text_to_ipa\n",
    "import markdown\n",
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Remove punctuation and normalize text for comparison\"\"\"\n",
    "    return re.sub(r'[^\\w\\s]', '', text).strip().lower()\n",
    "\n",
    "def highlight_differences(reference, actual):\n",
    "    \"\"\"Highlight differences between reference and actual transcriptions\"\"\"\n",
    "    diff = difflib.SequenceMatcher(None, reference, actual)\n",
    "    highlighted_reference = []\n",
    "    highlighted_actual = []\n",
    "\n",
    "    for tag, i1, i2, j1, j2 in diff.get_opcodes():\n",
    "        ref_segment = reference[i1:i2]\n",
    "        act_segment = actual[j1:j2]\n",
    "\n",
    "        if tag == 'equal':\n",
    "            highlighted_reference.append(ref_segment)\n",
    "            highlighted_actual.append(act_segment)\n",
    "        elif tag in ('replace', 'delete'):\n",
    "            highlighted_reference.append(f\" **{ref_segment}** \")\n",
    "        if tag in ('replace', 'insert'):\n",
    "            highlighted_actual.append(f\" **{act_segment}** \")\n",
    "\n",
    "    return \"\".join(highlighted_reference), \"\".join(highlighted_actual)\n",
    "\n",
    "def highlight_mismatched_characters(real_text, matched_text):\n",
    "    \"\"\"Highlight mismatched characters or words in the text.\"\"\"\n",
    "    highlighted_text = []\n",
    "    for real_char, matched_char in zip(real_text, matched_text):\n",
    "        if real_char != matched_char:\n",
    "            highlighted_text.append(f\" **{real_char}** \")\n",
    "        else:\n",
    "            highlighted_text.append(real_char)\n",
    "    # Append any remaining characters (if lengths differ)\n",
    "    if len(real_text) > len(matched_text):\n",
    "        highlighted_text.extend(f\" **{char}** \" for char in real_text[len(matched_text):])\n",
    "    return \"\".join(highlighted_text)\n",
    "\n",
    "def display_results(data):\n",
    "    \"\"\"Generate the result with mismatched characters highlighted.\"\"\"\n",
    "    real_transcripts = data[\"real_transcripts\"]\n",
    "\n",
    "    # Generate matched transcripts and IPA using Whisper and eng_to_ipa\n",
    "    transcribed_text = transcribe_audio_whisper(data[\"audio_path\"])[0]\n",
    "    matched_transcripts = preprocess_text(transcribed_text)\n",
    "\n",
    "    real_transcripts_ipa = text_to_ipa(real_transcripts)\n",
    "    matched_transcripts_ipa = text_to_ipa(matched_transcripts)\n",
    "\n",
    "    # Highlight mismatched characters in text and IPA\n",
    "    highlighted_text = highlight_mismatched_characters(real_transcripts, matched_transcripts)\n",
    "    highlighted_ipa = highlight_differences(real_transcripts_ipa, matched_transcripts_ipa)[1]\n",
    "\n",
    "    # Format the result\n",
    "    result = f\"\"\"\n",
    "### Pronunciation Feedback\n",
    "\n",
    "#### Text with Mistakes Highlighted:\n",
    "{highlighted_text}\n",
    "\n",
    "#### Correct IPA:\n",
    "/ {real_transcripts_ipa} /\n",
    "\n",
    "#### Transcribed IPA:\n",
    "/ {highlighted_ipa} /\n",
    "\"\"\"\n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example input JSON\n",
    "    json_input = {\n",
    "        \"real_transcripts\": \"why hello here\",\n",
    "        \"audio_path\": \"./IELTS_PracticeAndEvaluation/test_data/why-hello-there-103596.wav\"  # Replace with the path to the user's audio file\n",
    "    }\n",
    "\n",
    "    # Display pronunciation evaluation\n",
    "    markdown_result = display_results(json_input)\n",
    "    print(markdown_result)\n",
    "\n",
    "    # Save to Markdown file\n",
    "    with open(\"pronunciation_evaluation.md\", \"w\") as f:\n",
    "        f.write(markdown_result)\n",
    "\n",
    "    print(\"Pronunciation evaluation saved to pronunciation_evaluation.md\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pronunciation_feedback': {'text_with_mistakes_highlighted': 'why hello  **h**  **e**  **r**  **e** ',\n",
       "  'correct_ipa': 'waɪ hɛˈloʊ hir',\n",
       "  'transcribed_ipa': 'waɪ hɛˈloʊ  **ðɛ** r'}}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transcribe_audio_whisper(audio_path):\n",
    "    \"\"\"Simulated function for transcribing audio using Whisper model (replace with actual implementation).\"\"\"\n",
    "    # Implement actual transcription code here\n",
    "    return [\"why hello there\"]  # Dummy transcription for now\n",
    "\n",
    "def display_results(data):\n",
    "    \"\"\"Generate the result with mismatched characters highlighted.\"\"\"\n",
    "    real_transcripts = data[\"real_transcripts\"]\n",
    "\n",
    "    # Generate matched transcripts and IPA using Whisper and eng_to_ipa\n",
    "    transcribed_text = transcribe_audio_whisper(data[\"audio_path\"])[0]\n",
    "    matched_transcripts = preprocess_text(transcribed_text)\n",
    "\n",
    "    real_transcripts_ipa = text_to_ipa(real_transcripts)\n",
    "    matched_transcripts_ipa = text_to_ipa(matched_transcripts)\n",
    "\n",
    "    # Highlight mismatched characters in text and IPA\n",
    "    highlighted_text = highlight_mismatched_characters(real_transcripts, matched_transcripts)\n",
    "    highlighted_ipa = highlight_differences(real_transcripts_ipa, matched_transcripts_ipa)[1]\n",
    "\n",
    "    # Return results as JSON\n",
    "    result = {\n",
    "        \"pronunciation_feedback\": {\n",
    "            \"text_with_mistakes_highlighted\": highlighted_text,\n",
    "            \"correct_ipa\": real_transcripts_ipa,\n",
    "            \"transcribed_ipa\": highlighted_ipa\n",
    "        }\n",
    "    }\n",
    "    return result\n",
    "\n",
    "res = display_results(json_input)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transcript:  Why hello there.\n",
    "\n",
    "### Pronunciation Feedback\n",
    "\n",
    "#### Text with Mistakes Highlighted:\n",
    "why hello  **h**  **e**  **r**  **e** \n",
    "\n",
    "#### Correct IPA:\n",
    "/ waɪ hɛˈloʊ hir /\n",
    "\n",
    "#### Transcribed IPA:\n",
    "/ waɪ hɛˈloʊ  **ðɛ** r /"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pronunc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
